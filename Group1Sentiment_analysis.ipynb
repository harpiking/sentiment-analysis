{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad709a0d-105a-4722-a988-4a8dc6ebd5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Happy\n",
      "[nltk_data]     Iguare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "#from transformers import GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51cf3b-0fdc-459d-be9b-b30ae346c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe10f7-2d28-4853-a7c3-6ab60f53afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, TensorDataset\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba63c07-15fc-4e33-9885-e022516d216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text sentiment  \\\n",
      "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
      "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
      "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
      "3  01082688c6                                        happy bday!  positive   \n",
      "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
      "\n",
      "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
      "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
      "1          noon       21-30      Albania         2877797.0          27400.0   \n",
      "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
      "3       morning       46-60      Andorra           77265.0            470.0   \n",
      "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
      "\n",
      "   Density (P/Km²)  \n",
      "0             60.0  \n",
      "1            105.0  \n",
      "2             18.0  \n",
      "3            164.0  \n",
      "4             26.0  \n",
      "Original train data length: 27481\n",
      "Original test data length: 4815\n",
      "Number of neutral sentiments in training data: 11117\n",
      "Number of neutral sentiments in testing data: 1430\n",
      "Updated train data length: 16363\n",
      "Updated test data length: 2104\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv', encoding='ISO-8859-1')\n",
    "train_data = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(test_data.head())\n",
    "# Print the original lengths\n",
    "print(\"Original train data length:\", len(train_data))\n",
    "print(\"Original test data length:\", len(test_data))\n",
    "\n",
    "# Drop rows with missing 'text'\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "test_data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Count 'neutral' sentiments before filtering them out\n",
    "neutral_train_count = train_data[train_data['sentiment'] == 'neutral'].shape[0]\n",
    "neutral_test_count = test_data[test_data['sentiment'] == 'neutral'].shape[0]\n",
    "\n",
    "print(\"Number of neutral sentiments in training data:\", neutral_train_count)\n",
    "print(\"Number of neutral sentiments in testing data:\", neutral_test_count)\n",
    "\n",
    "# Define sentiment mapping\n",
    "sentiment_mapping = {\n",
    "    'negative': 0,\n",
    "    'positive': 1,\n",
    "    'neutral': 2,\n",
    "}\n",
    "\n",
    "# Apply the mapping to the sentiment column in both DataFrames\n",
    "train_data['sentimentLabel'] = train_data['sentiment'].map(sentiment_mapping)\n",
    "test_data['sentimentLabel'] = test_data['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Drop any rows that failed to map\n",
    "train_data.dropna(subset=['sentimentLabel'], inplace=True)\n",
    "test_data.dropna(subset=['sentimentLabel'], inplace=True)\n",
    "\n",
    "# Filter out the 'neutral' sentiments\n",
    "train_data = train_data[train_data['sentimentLabel'] != 2]\n",
    "test_data = test_data[test_data['sentimentLabel'] != 2]\n",
    "\n",
    "# Simplify the datasets to contain only 'text' and 'encoded sentiment'\n",
    "train = train_data[['text', 'sentimentLabel']]\n",
    "test = test_data[['text', 'sentimentLabel']]\n",
    "\n",
    "# Print the updated lengths\n",
    "print(\"Updated train data length:\", len(train))\n",
    "print(\"Updated test data length:\", len(test))\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train['text']\n",
    "y_train = train['sentimentLabel']\n",
    "X_test = test['text']\n",
    "y_test = test['sentimentLabel']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45f56d-dee6-42b5-b78b-eaba64a6f7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dea2703-abb0-4369-ae11-b07a24bb3180",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.90      0.88      1001\n",
      "    positive       0.90      0.88      0.89      1103\n",
      "\n",
      "    accuracy                           0.89      2104\n",
      "   macro avg       0.89      0.89      0.89      2104\n",
      "weighted avg       0.89      0.89      0.89      2104\n",
      "\n",
      "Training completed in 293.77 seconds.\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in text.split()]) for text in texts]\n",
    "\n",
    "# Enhanced pipeline with VotingClassifier\n",
    "start_time = time.time()\n",
    "pipeline = Pipeline([\n",
    "    ('lemmatize', FunctionTransformer(lambda x: lemmatize_text(x), validate=False)),\n",
    "    ('tfidf', TfidfVectorizer(lowercase=True, max_features=5000)),\n",
    "    ('clf', VotingClassifier(estimators=[\n",
    "        ('svc', SVC(probability=True, kernel='linear', C=1.0)),\n",
    "        ('mnb', MultinomialNB(alpha=1.0))\n",
    "    ], voting='soft')),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__min_df': [5],\n",
    "    'tfidf__max_df': [0.75],\n",
    "    'clf__svc__C': [1.0],\n",
    "    'clf__mnb__alpha': [1.0,10.0],\n",
    "}\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search with the updated pipeline and StratifiedKFold\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions and classification report\n",
    "pred = best_model.predict(X_test)\n",
    "print(\"Enhanced Classification Report\")\n",
    "print(classification_report(y_test, pred, target_names=['negative', 'positive']))\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3f556-c9ae-49f6-a0c9-d227f5587528",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', lowercase=True)),  # Use built-in English stop words\n",
    "    ('svd', TruncatedSVD(n_components=100)),  # Reducing dimensions\n",
    "    ('clf', SVC(probability=True, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "svm_param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__min_df': [3],\n",
    "    'tfidf__max_df': [0.75],\n",
    "    'svd__n_components': [100],\n",
    "    'clf__C': [0.1, 1],\n",
    "    'clf__kernel': ['linear'],\n",
    "    'clf__gamma': ['scale'],\n",
    "}\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "svm_grid_search = GridSearchCV(svm_pipeline, svm_param_grid, cv=kfolds, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "svm_best_model = svm_grid_search.best_estimator_\n",
    "svm_pred = svm_best_model.predict(X_test)\n",
    "\n",
    "print(\"Optimized SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred, target_names=['negative', 'positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277fdce-c5aa-4370-9e89-fce121538bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb8937c-0b9f-4feb-b9bf-6f2311db7d41",
   "metadata": {
    "tags": []
   },
   "source": [
    "**bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e03e561-0db1-4de8-baf6-fcea47235dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14021682739257812\n",
      "Epoch 2, Loss: 0.07911890000104904\n",
      "Epoch 3, Loss: 0.00796283408999443\n",
      "Total execution time: 1063.79 seconds\n"
     ]
    }
   ],
   "source": [
    "def encode_reviews(tokenizer, reviews, max_length=64):\n",
    "    return tokenizer(list(reviews), padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = encode_reviews(tokenizer, X_train)\n",
    "test_encodings = encode_reviews(tokenizer, X_test)\n",
    "\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * 3  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "for epoch in range(3):  \n",
    "    for batch in train_loader:\n",
    "        batch = tuple(t.to(model.device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = tuple(t.to(model.device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        true_labels.extend(batch[2].tolist())\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcd7952-3216-4291-8425-058b9ccec6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9401    0.9570    0.9485      1001\n",
      "    positive     0.9604    0.9447    0.9525      1103\n",
      "\n",
      "    accuracy                         0.9506      2104\n",
      "   macro avg     0.9503    0.9509    0.9505      2104\n",
      "weighted avg     0.9507    0.9506    0.9506      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_report = classification_report(true_labels, predictions, target_names=['negative', 'positive'], digits=4)\n",
    "print(\"Classification Report:\\n\", bert_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c08bb-53b7-4b81-83c8-39a928669df8",
   "metadata": {},
   "source": [
    "**gpt2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d6a9bc9-3c68-44b1-be4e-68c7c4cad2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.16836963593959808\n",
      "Epoch 2, Loss: 0.512739360332489\n",
      "Epoch 3, Loss: 0.20878435671329498\n",
      "Total execution time: 5201.76 seconds\n"
     ]
    }
   ],
   "source": [
    "def encode_reviews(tokenizer, reviews, max_length=64):\n",
    "    return tokenizer(list(reviews), padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Initialize GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "train_encodings = encode_reviews(tokenizer, X_train)\n",
    "test_encodings = encode_reviews(tokenizer, X_test)\n",
    "\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=3)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * 3 \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "for epoch in range(3):  \n",
    "    for batch in train_loader:\n",
    "        batch = tuple(t.to(model.device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "gpt_predictions, gpt_true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = tuple(t.to(model.device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        gpt_predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        gpt_true_labels.extend(batch[2].tolist())\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "accuracy = accuracy_score(gpt_true_labels, gpt_predictions)\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eafbf0f9-ed58-41be-a1d6-3008cd107982",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9289    0.9391    0.9339      1001\n",
      "    positive     0.9441    0.9347    0.9394      1103\n",
      "\n",
      "    accuracy                         0.9368      2104\n",
      "   macro avg     0.9365    0.9369    0.9367      2104\n",
      "weighted avg     0.9369    0.9368    0.9368      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_report = classification_report(gpt_true_labels, gpt_predictions, target_names=['negative', 'positive'], digits=4)\n",
    "print(\"Classification Report:\\n\", gpt_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6bf0b-d4a5-4148-a62c-a6a86c08f24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
